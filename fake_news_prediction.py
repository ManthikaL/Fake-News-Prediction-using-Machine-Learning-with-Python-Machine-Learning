# -*- coding: utf-8 -*-
"""Fake News Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OL04TVGKwNpxdoiDB-N8CxVx9pRkkDKS

Artificial intelligence (AI) has advanced significantly in a number of application areas. One area in which AI is particularly important is the detection of false news. The presented code demonstrates how artificial intelligence (AI) approaches can be applied to the problem of identifying false news from authentic news based on textual content. Preprocessing the data, applying machine learning models for categorization, and making use of natural language processing (NLP) technologies are all part of this research.

First, import necessary libraries (such NumPy, Pandas, and scikit-learn) and Natural Language Toolkit (NLTK) for Natural Language Processing (NLP) operations. The code for text preparation makes use of NLTK's useful tools, such as stopwords and a Porter Stemmer. Common words like "the" and "is," which are stopwords, are eliminated to concentrate on words that convey meaning. Furthermore, by reducing words to their root form, stemming facilitates feature extraction and lowers dimensionality.

The dataset is made up of news articles with identified categories (actual or bogus), loaded using Pandas. To deal with the missing values, empty strings are inserted into them. A unified 'content' column is subsequently created by combining the author name and news title. The ensuing NLP tasks, including stemming, are made easier by this consolidation and are implemented via a bespoke function. The function removes non-alphabetic letters, tokenizes the text, lowercases it, stems the remaining words, and keeps stopwords out.

After text preprocessing, the data is separated into labels (Y) and features (X). The labels specify whether the news is authentic (1) or fraudulent (0), whereas the characteristics show the processed textual content. The vectorization technique known as TF-IDF (Term Frequency-Inverse Document Frequency) is utilized to transform the textual data into a format that is appropriate for machine learning. TF-IDF assists in producing a numerical depiction of the text by determining a word's significance in a document based on how frequently it appears in various documents.

The train test split function is then used to split the dataset into training and testing sets. Because of its ease of use and efficiency in binary classification problems, a logistic regression model is selected for classification. Accuracy scores on the training and test sets are used to assess the model's performance after it has been trained on the training data.
The AI techniques applied in this code represent a holistic approach to fake news detection:

Text Preprocessing: By focusing on the core of the content and eliminating noise, the usage of NLTK tools for stemming and stopword removal improves the textual features.

TF-IDF Vectorization: This method captures the significance of words in differentiating between authentic and fraudulent news by converting textual data into a numerical representation. It solves the problem of machine learning models having to deal with unstructured text.

Suitable for binary classification applications, logistic regression is a supervised learning technique that was selected as the classification algorithm. It uses the TF-IDF-transformed features to model the likelihood that a news article is fraudulent.

Train-Test Split and Evaluation: To train the model on one subset and assess its performance on another, the dataset is split. The accuracy scores offer a numerical representation of the model's ability to differentiate between authentic and fraudulent news.

In order to handle the changing nature of information distribution, artificial intelligence (AI) approaches for fake news identification are essential. Together, TF-IDF vectorization, logistic regression, and text preprocessing provide a strong framework for detecting disinformation, supporting larger initiatives to uphold information integrity in the digital era. These methods will probably be improved upon and incorporated into more complex models for improved fake news identification as AI develops.
"""

# Importing necessary libraries
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Downloading stopwords for English language from NLTK
import nltk
nltk.download('stopwords')

# Printing the stopwords in English
print(stopwords.words('english'))

# Loading the dataset into a pandas DataFrame
news_dataset = pd.read_csv('/content/train.csv')

"""The solution that is being offered is mainly concerned with text classification through the use of a logistic regression model. The input requirements are based around a CSV file containing labels. It is anticipated that the dataset, named "train.csv," will include columns like "author," "title," and "label," where "label" indicates the veracity of news stories (0 for true, 1 for fraudulent). A new column named "content" is created by combining the data from the fields labeled "author" and "title." We presume 
that the data came from the CSV file '/content/train.csv.' The dataset must be appropriately prepared with the right column names, data labels, and any missing values must be handled by substituting empty strings for them. Preprocessing operations are performed on the text, such as stopword removal and stemming, in order to enable efficient feature extraction. Furthermore, the textual input is transformed into numerical vectors using the TF-IDF vectorizer. Next, for model evaluation, the dataset is divided into 
training and testing sets. To summarize, the implementation needs a well-organized CSV file with designated columns as input, and handling missing values and using text preprocessing techniques are part of the data preparation process."""

# Displaying the shape of the dataset
news_dataset.shape

# Displaying the first 5 rows of the dataframe
news_dataset.head()

# Counting the number of missing values in the dataset
news_dataset.isnull().sum()

# Replacing the null values with an empty string
news_dataset = news_dataset.fillna('')

# Merging the author name and news title to create a new 'content' column
news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']

# Displaying the 'content' column
print(news_dataset['content'])

# Separating the data and labels
X = news_dataset.drop(columns='label', axis=1)
Y = news_dataset['label']

# Displaying the data and labels
print(X)
print(Y)

"""Stemming is the process of reducing a word to its Root word"""

# Initializing a Porter Stemmer for word stemming
port_stem = PorterStemmer()

# Function to perform stemming on the 'content' column
def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

# Applying stemming to the 'content' column
news_dataset['content'] = news_dataset['content'].apply(stemming)

# Displaying the 'content' column after stemming
print(news_dataset['content'])

# Separating the data and labels after stemming
X = news_dataset['content'].values
Y = news_dataset['label'].values

"""Displaying the data and labels after stemming"""

print(X)

print(Y)

# Displaying the shape of labels
Y.shape

# Converting the textual data to numerical data using TF-IDF Vectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit(X)
X = vectorizer.transform(X)

# Displaying the transformed data
print(X)

"""Splitting the dataset to training & test data"""

# Splitting the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

"""Training the Model: Logistic Regression"""

# Initializing a Logistic Regression model
model = LogisticRegression()

# Training the model on the training data
model.fit(X_train, Y_train)

"""Evaluation (Accuracy Score)"""

# Calculating accuracy score on the training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

# Displaying the accuracy score on the training data
print('Accuracy score of the training data : ', training_data_accuracy)

# Calculating accuracy score on the test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

# Displaying the accuracy score on the test data
print('Accuracy score of the test data : ', test_data_accuracy)

"""Making a Predictive System"""

# Making a prediction on a specific example from the test data
X_new = X_test[1234]
prediction = model.predict(X_new)
print("Prediction for the example:", prediction)

# Displaying the result of the prediction
if prediction[0] == 0:
    print('The news is Real')
else:
    print('The news is Fake')

"""The solution that is being offered is a text classification model that makes use of logistic regression to forecast whether news articles are authentic or not. Preprocessing the data includes addressing missing values, combining pertinent columns, and using stemming to condense words to their most basic form. TF-IDF vectorization is then used to convert the textual data into numerical vectors. A logistic regression model is trained and assessed using the training and testing sets of the dataset. The model's prediction for a particular test instance is presented, together with the accuracy scores for the training and testing data. Preprocessing, TF-IDF vectorization, model training, and evaluation all seem to have been carried out appropriately when compared to the implementation and expected/correct outcomes. The model's performance on training and testing data is revealed by the accuracy scores, and the prediction on a new occurrence is suitably classified as true or fake. Overall, the implementation shows that logistic regression can be successfully applied for predicting fake news, in line with expectations for a text classification problem."""

# Validate the actual label for the example from the test data
print("Actual label for the example:", Y_test[1234])

"""A logistic regression model is implemented by the provided code to categorize news articles as genuine or fraudulent according to their content. Preprocessing the dataset involves addressing missing values, combining pertinent columns, and using stemming to condense words to their most basic form. TF-IDF vectorization is then used to convert the textual data into numerical vectors. A logistic regression model is trained on the training set of the dataset after it has been divided into training and testing sets. To assess the model's performance, the accuracy scores for the training and testing sets are calculated. The evaluation's findings show how well the model applies to previously untested data. In this particular instance, the model's efficacy is demonstrated by the printing of the accuracy scores on both training and testing data. Furthermore, a fresh instance from the test set is subjected to the model, and the prediction and the actual label are contrasted. The evaluation of the code effectively shows how accurate the logistic regression model is at identifying news articles as authentic or fraudulent using the dataset that is supplied."""
